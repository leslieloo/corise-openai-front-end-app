{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Inverse Reinforcement Learning Without RL with Gokul Swamy - #643", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone. Welcome to another episode of the TwiML AI podcast. I'm your host, Sam Charrington, and today I'm joined by Gokul Swamy. Gokul is a PhD student at the Robotics Institute at Carnegie Mellon University. Wherever you're listening to today's show, make sure you're subscribed and be sure to like, rate, and review the show. Gokul, welcome to the podcast. Yeah. Thank you so much for having me. Hey, I'm looking forward to digging into our conversation. We'll be talking about your research into the fields of efficient interactive learning and making good decisions without observable boundaries, with a particular emphasis on a few of the papers that you are presenting at this year's ICML conference. Before we dig into that though, I'd love to have you share a little bit about your background and how you came to the field. Yeah, good question. So I just wrapped up my third year at CMU. I spent a bunch of time there working on different things that are about sort of algorithms for this field called imitation learning, which is broadly speaking about how you can try and learn to make good decisions from data. Before that, I spent a few years at Berkeley where I was a master's and undergrad student, and there I was working on methods for human-robot interaction. Even before that, I grew up in San Diego, where I tried to just maximize the amount of time I spent on the beach. Sounds worthwhile. Yeah. So tell us a little bit about your research interests and the focus of your work. Yeah. So I think over the last few years, what I've been really trying to think about is how we can try and learn to make sequence of decisions well from observing data, some sort of expert demonstrated during the same thing. So perhaps the most intuitive example of this is something like self-driving cars. So you put some person in a car, we strap the car up with sensors, we get them to drive all around Pittsburgh, and then we want to learn a program, let's the car do the same thing. I spent a lot of time thinking about what are the right sorts of algorithms for that problem. I think there's two parts of that that I'm most interested in. The first is how we do this efficiently. I mean efficiently in various senses, both in terms of the amount of data you need to use, the amount of compute you need to use, stuff like that. The other thing I'm really interested in is if we're trying to get a car to do the same thing as a person, they don't have the same observation space in some sense. They don't see exactly the same way. They don't have the exact same sensors. And at that point, there's all these sorts of things that show up that affect a relationship you care about that you don't really observe. So let's say you're trying to predict from some variable X to some variable Y, and there's some unobserved variable U that affects both. This is usually called an unobserved confounder. And here I'm trying to predict from some sort of observations, some sort of how much I want to the steering wheel, how much I want to press the gas pedal, stuff like that. For example, your self-driving car might not be able to pick up on the fact that a hand gesture from somebody in the car across from them actually means something. And when you kind of have this sort of partial observability, I think it's a really interesting question of how do you still learn well? So I've been thinking about both in the sort of idealized setting, how do you do it efficiently, and in the sort of more real world setting where you don't perhaps get the same access to the same pieces of information, how do you make decisions well there? Awesome. And how does that tie into the research that you're presenting at the conference this year? Yeah. So I think we have a few papers at the conference, which I'm very thankful for, and they sort of touch on different parts of those topics. So we have one paper at the main conference, which is really focused on the question of how do we do imitation"}, "podcast_summary": "This is an interview with Gokul Swamy, a PhD student at Carnegie Mellon University. He discusses his research on efficient interactive learning and making good decisions without observable boundaries. He talks about his background and how he got into the field, as well as his research interests and the focus of his work. He also mentions some papers he will be presenting at the ICML conference that delve into these topics.", "podcast_guest": {"name": "Gokul Swamy", "summary": "None Carnegie Mellon University"}, "podcast_highlights": "- Host: Sam Charrington welcomes Gokul Swamy to the TwiML AI podcast\n- Gokul is a PhD student at the Robotics Institute at Carnegie Mellon University\n- Host encourages listeners to subscribe, like, rate, and review the show\n- Gokul discusses his background and experience at CMU and Berkeley\n- Gokul's research focuses on learning to make good decisions from data, particularly in the field of imitation learning\n- He is interested in efficient algorithms for this problem and dealing with unobserved variables that affect decision-making\n- Gokul has multiple papers at this year's ICML conference that touch on these topics, including one on imitation learning."}